{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imr2VZ2W5jGo",
        "outputId": "bfb331d1-aef1-46bc-bf2e-c2cc22984dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Î∂ÑÌè¨ ÌõÑ: Counter({True: 71961, False: 71961})\n",
            "R¬≤ score: -0.2027\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "df = pd.read_csv('employee.csv')\n",
        "\n",
        "# ÌÉÄÍ≤üÍ≥º ÌîºÏ≤ò Î∂ÑÎ¶¨\n",
        "target_col = 'Resigned'\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "# Î≤îÏ£ºÌòï Îç∞Ïù¥ÌÑ∞ ÎùºÎ≤® Ïù∏ÏΩîÎî©\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "\n",
        "# SMOTE Ïò§Î≤ÑÏÉòÌîåÎßÅ\n",
        "\n",
        "# NearMiss Ïñ∏ÎçîÏÉòÌîåÎßÅ\n",
        "#nearmiss = NearMiss()\n",
        "#X_balanced, y_balanced = nearmiss.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "# ÌïôÏäµ/ÌÖåÏä§Ìä∏ Î∂ÑÌï†\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. ÎûúÎç§ Ïò§Î≤ÑÏÉòÌîåÎßÅ Ï†ÅÏö©(ÏÜåÏàò ÌÅ¥ÎûòÏä§ ÎäòÎ¶¨Í∏∞)\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "print('Train Î∂ÑÌè¨ ÌõÑ:', Counter(y_train_res))\n",
        "\n",
        "# ÌååÏù¥ÌîÑÎùºÏù∏ Íµ¨ÏÑ± Î∞è ÌïôÏäµ\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', MLPRegressor(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42))\n",
        "])\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# ÌèâÍ∞Ä\n",
        "y_pred = pipeline.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R¬≤ score: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import f1_score, mean_squared_error\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "df = pd.read_csv('employee.csv')\n",
        "\n",
        "# 2. Ï†ÑÏ≤òÎ¶¨\n",
        "df = df.dropna()  # Í≤∞Ï∏°Í∞í Ï†úÍ±∞\n",
        "\n",
        "# Label Encoding for categorical columns\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Target Î∂ÑÎ¶¨\n",
        "X = df.drop('Resigned', axis=1)\n",
        "y = df['Resigned'].astype(int)\n",
        "\n",
        "# 3. SMOTE Ïò§Î≤ÑÏÉòÌîåÎßÅ\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# 4. Îç∞Ïù¥ÌÑ∞ Ïä§ÏºÄÏùºÎßÅ Î∞è Î∂ÑÌï†\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_resampled)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
        ")\n",
        "\n",
        "# 5. Torch Tensor Î≥ÄÌôò\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "\n",
        "\n",
        "# 6. Îî•Îü¨Îãù Î™®Îç∏ Ï†ïÏùò\n",
        "class ResignPredictorV2(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ResignPredictorV2, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.act1 = nn.LeakyReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.act2 = nn.LeakyReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.act3 = nn.LeakyReLU()\n",
        "\n",
        "        self.fc_out = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        x = self.act3(self.bn3(self.fc3(x)))\n",
        "        return self.fc_out(x)  # No sigmoid!\n",
        "\n",
        "\n",
        "model = ResignPredictorV2(X_train.shape[1])\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# 6. EarlyStopping Íµ¨ÌòÑ\n",
        "patience = 10  # Í≤ÄÏ¶ù ÏÜêÏã§ Í∞úÏÑ† ÏóÜÏùÑ Îïå Í∏∞Îã§Î¶¨Îäî ÏµúÎåÄ epoch Ïàò\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "num_epochs = 2000  # ÏµúÎåÄ ÏóêÌè≠ Ïàò\n",
        "\n",
        "# 7. ÌïôÏäµ\n",
        "epochs = 3000\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 150 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(X_test_tensor)\n",
        "    probs = torch.sigmoid(logits).numpy()\n",
        "    preds = (probs > 0.5).astype(int)\n",
        "\n",
        "f1 = f1_score(y_test, preds)\n",
        "\n",
        "\n",
        "print(f\"\\nüìä F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlir5SL65w0C",
        "outputId": "e0aa097e-f98e-439d-fa9b-2ca81aabc807"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 150/3000, Loss: 0.4303\n",
            "Epoch 300/3000, Loss: 0.3576\n",
            "Epoch 450/3000, Loss: 0.3259\n",
            "Epoch 600/3000, Loss: 0.3050\n",
            "Epoch 750/3000, Loss: 0.2902\n",
            "Epoch 900/3000, Loss: 0.2796\n",
            "Epoch 1050/3000, Loss: 0.2719\n",
            "Epoch 1200/3000, Loss: 0.2644\n",
            "Epoch 1350/3000, Loss: 0.2587\n",
            "Epoch 1500/3000, Loss: 0.2531\n",
            "Epoch 1650/3000, Loss: 0.2498\n",
            "Epoch 1800/3000, Loss: 0.2474\n",
            "Epoch 1950/3000, Loss: 0.2432\n",
            "Epoch 2100/3000, Loss: 0.2400\n",
            "Epoch 2250/3000, Loss: 0.2375\n",
            "Epoch 2400/3000, Loss: 0.2359\n",
            "Epoch 2550/3000, Loss: 0.2344\n",
            "Epoch 2700/3000, Loss: 0.2322\n",
            "Epoch 2850/3000, Loss: 0.2302\n",
            "Epoch 3000/3000, Loss: 0.2296\n",
            "\n",
            "üìä F1 Score: 0.8179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_test, preds, target_names=[\"Ïû¨ÏßÅÏûê(0)\", \"Ìá¥ÏÇ¨Ïûê(1)\"])\n",
        "print(\"\\nüìã Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uByrGuR6V4D",
        "outputId": "6d9ebeb5-229b-43f0-d8b4-7ee56e3e7db4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìã Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Ïû¨ÏßÅÏûê(0)       0.83      0.79      0.81     17998\n",
            "      Ìá¥ÏÇ¨Ïûê(1)       0.80      0.84      0.82     17998\n",
            "\n",
            "    accuracy                           0.81     35996\n",
            "   macro avg       0.81      0.81      0.81     35996\n",
            "weighted avg       0.81      0.81      0.81     35996\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-9u8-_Sa6ps5"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}